---
title: "MM Group - Research"
layout: textlay
excerpt: "MM Group -- Research"
sitemap: false
permalink: /research/
---

# Overview

Our research interests mainly include multimedia analysis and computing, computer vision, machine learning, and artificial intelligence. Recently, we are focusing on the visual understanding via deep learning, e.g., video/image recognition, detection and segmentation, video/image captioning, and video/image question answering (QA). We also explore the deep learning methods’ vulnerability and its robustness to adversarial attacks. Our goal is to further understand the vulnerability and interpretability of deep learning methods, which will provide theoretic evidences and methodology strategies for constructing a safer and more reliable system of image semantic understanding.

# Highlights

### 1. Object Dectection in Practical Scenes: Domain Adaptation and Few Samples

|  |  |
| --- | --- |
| ![]({{ site.url }}{{ site.baseurl }}/images/respic/domain_adap/iiod_result2.png){: style="width: 350px; float: left; border: 10px"} | **1 Instance-Invariant Domain Adaptive Object Detection via Progressive Disentanglement** <br>Aming Wu, Yahong Han, Linchao Zhu, Yi Yang <br>IEEE TPAMI, DOI: 10.1109/TPAMI.2021.3060446 <a href='https://github.com/AmingWu/IIOD'>(Project Page)</a>  <br>In this work, a progressive disentangled framework is  proposed to solve domain adaptive object detection for the first time. Particularly, base on disentangled learning used for feature decomposition, we devise two disentangled layers to decompose domain-invariant and domain-specific features. And the instance-invariant features are extracted based on the domain-invariant features. Finally, to enhance the disentanglement, a three-stage training mechanism including multiple loss functions is devised to optimize our model. The proposed method can achieve excellent detection performance in night and fog domain adaptive object detection in real road scenes under different weather conditions.  <br> <br>|
| ![]({{ site.url }}{{ site.baseurl }}/images/respic/domain_adap/vdd.png){: style="width: 350px; float: left; border: 10px"} | **2 Vector-Decomposed Disentanglement for Domain-Invariant Object Detection** <br>Aming Wu, Rui Liu, Yahong Han, Linchao Zhu, Yi Yang <br>ICCV 2021 <a href='https://github.com/AmingWu/VDD-DAOD'>(Project Page)</a>  <br> To improve the generalization of detectors, for domain adaptive object detection (DAOD), a novel disentangled method based on vector decomposition is proposed to disentangle domain-invariant representations from domain-specific representations for the first time. Firstly, an extractor is devised to separate domain-invariant representations from the input, which are used for extracting object proposals. Secondly, domain-specific representations are introduced as the differences between the input and domain-invariant representations. Through the difference operation, the gap between the domain-specific and domain-invariant representations is enlarged, which promotes domain-invariant representations to contain more domain-irrelevant information. The proposed method can achieve outstanding performance in the all-weather cross-domain and multi-severe weather mixed domain target detection, such as in fog, dusk rain and night rain domain adaptive object detection in real road scenes.<br> <br>|
| ![]({{ site.url }}{{ site.baseurl }}/images/respic/domain_adap/upfsod.png){: style="width: 350px; float: left; border: 10px"} | **3 Bidirectional Adversarial Training for Semi-Supervised Domain Adaptation** <br>Aming Wu, Yahong Han, Linchao Zhu, Yi Yang <br>ICCV 2021 <a href='https://github.com/AmingWu/UP-FSOD'>(Project Page)</a> <br> In this paper, we explore how to enhance object features with intrinsical characteristics that are universal across different object categories in few-shot object detection (FSOD). We propose a new prototype, namely universal prototype, that is learned from all object categories. Besides the advantage of characterizing invariant characteristics, the universal prototypes alleviate the impact of unbalanced object categories. After enhancing object features with the universal prototypes, we impose a consistency loss to maximize the agreement between the enhanced features and the original ones, which is beneficial for learning invariant object characteristics. Thus, we develop a new framework of few-shot object detection with universal prototypes (FSODup) that owns the merit of feature generalization towards novel objects. <br> <br>|

### 2. Vision-to-Language: Understanding and Reasoning

|  |  |
| --- | --- |
| ![]({{ site.url }}{{ site.baseurl }}/images/respic/video_cap/mmvc.png){: style="width: 350px; float: left; border: 10px"} | **1 Multirate Multimodal Video Captioning**  <br>Ziwei Yang, Youjiang Xu, Huiyun Wang, Bo Wang, Yahong Han <br>ACM Multimedia 2017 (<b>ACM MM 2017 Grand Challenge Honorable Mention Award</b>)<br> <a href='http://ms-multimedia-challenge.com/2017/challenge#video'>The Runner-up of the 2nd MSR Video to Language Challenge</a> <br>In this paper, we propose a Multirate Multimodal Approach for video captioning. Considering that the speed of motion in videos varies constantly, we utilize a Multirate GRU to capture temporal structure of videos. It encodes video frames with different intervals and has a strong ability to deal with motion speed variance. As videos contain different modality cues, we design a particular multimodal fusion method. By incorporating visual, motion, and topic information together, we construct a well-designed video representation. Then the video representation is fed into a RNN-based language model for generating natural language descriptions. We evaluate our approach for video captioning on "Microsoft Research - Video to Text" (MSR-VTT), a large-scale video benchmark for video understanding. And our approach gets great performance on the 2nd MSR Video to Language Challenge.<br> <br>|
| ![]({{ site.url }}{{ site.baseurl }}/images/respic/video_cap/VCHP.png){: style="width: 350px; float: left; border: 10px"} | **2 Video Interactive Captioning with Human Prompts** <br>Aming Wu, Yahong Han, Yi Yang <br>IJCAI 2019 <a href='https://github.com/ViCap01/ViCap'>(Project Page)</a> <br>As a video often includes rich visual content and semantic details, different people may be interested in different views. Thus the generated sentence always fails to meet the ad hoc expectations. In this paper, we make a new attempt that, we launch a round of interaction between a human and a captioning agent. After generating an initial caption, the agent asks for a short prompt from the human as a clue of his expectation. Then, based on the prompt, the agent could generate a more accurate caption. We name this process a new task of video interactive captioning (ViCap). Taking a video and an initial caption as input, we devise the ViCap agent which consists of a video encoder, an initial caption encoder, and a refined caption generator. Experimental results not only show the prompts can help generate more accurate captions, but also demonstrate the good performance of the proposed method. <br> <br>|
| ![]({{ site.url }}{{ site.baseurl }}/images/respic/vqa/mqa.png){: style="width: 350px; float: left; border: 10px"} | **3 Movie Question Answering: Remembering the Textual Cues for Layered Visual Contents**  <br>Bo Wang, Youjiang Xu, Yahong Han, Richang Hong <br>AAAI 2018 <a href='https://github.com/bowong/Layered-Memory-Network'>(Project Page)</a> <br><a href='http://movieqa.cs.toronto.edu/workshops/iccv2017/'>Winner of the MovieQA and The Large Scale Movie Description Challenge (LSMDC) @ ICCV 2017</a> <br>Understanding movie stories through only visual content is still a hard problem. In this paper, for answering questions about movies, we put forward a Layered Memory Network (LMN) that represents frame-level and clip-level movie content by the Static Word Memory module and the Dynamic Subtitle Memory module, respectively. Particularly, we firstly extract words and sentences from the training movie subtitles. Then the hierarchically formed movie representations, which are learned from LMN, not only encode the correspondence between words and visual content inside frames, but also encode the temporal alignment between sentences and frames inside movie clips. We also extend our LMN model into three variant frameworks to illustrate the good extendable capabilities. The good performance successfully demonstrates that the proposed framework of LMN is effective and the hierarchically formed movie representations have good potential for the applications of movie question answering.<br> <br>|
| ![]({{ site.url }}{{ site.baseurl }}/images/respic/vqa/ccn.png){: style="width: 350px; float: left; border: 10px"} | **4 Connective Cognition Network for Directional Visual Commonsense Reasoning** <br>Aming Wu, Linchao Zhu, Yahong Han, Yi Yang <br>NeurIPS 2019 <a href='https://github.com/AmingWu/CCN'>(Project Page)</a> <br>Recent studies on neuroscience have suggested that brain function or cognition can be described as a global and dynamic integration of local neuronal connectivity, which is context-sensitive to specific cognition tasks. Inspired by this idea, towards Visual commonsense reasoning (VCR), we propose a connective cognition network (CCN) to dynamically reorganize the visual neuron connectivity that is contextualized by the meaning of questions and answers. Concretely, we first develop visual neuron connectivity to fully model correlations of visual content. Then, a contextualization process is introduced to fuse the sentence representation with that of visual neurons. Finally, based on the output of contextualized connectivity, we propose directional connectivity to infer answers or rationales. Experimental results on the VCR dataset demonstrate the effectiveness of our method.<br> <br>|

### 3. Adversarial Vision and Robustness: Towards AI Security

|  |  |
| --- | --- |
| ![]({{ site.url }}{{ site.baseurl }}/images/respic/adv/CurlsandWhey.png){: style="width: 350px; float: left; border: 10px"} | **1 Curls & Whey: Boosting Black-Box Adversarial Attacks**  <br>Yucheng Shi, Siyu Wang, Yahong Han <br>CVPR 2019, (Oral),<a href='https://github.com/walegahaha/Curls-Whey'>(Project Page)</a> <br> <b> Fourth place in both </b> <a href='https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge-untargeted-attack-track/leaderboards'>Untargeted Attack Track</a><b> and </b> <a href='https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge-targeted-attack-track/leaderboards'>Targeted Attack Track</a> <b>of NIPS 2018 Adversarial Vision Challenge </b> <br>In this work, we propose Curls & Whey black-box attack to fix the above two defects. During Curls iteration, by combining gradient ascent and descent, we ‘curl’ up iterative trajectories to integrate more diversity and transferability into adversarial examples. Curls iteration also alleviates the diminishing marginal effect in existing iterative attacks. The Whey optimization further squeezes the ‘whey’ of noises by exploiting the robustness of adversarial perturbation. Extensive experiments on Imagenet and Tiny-Imagenet demonstrate that our approach achieves impressive decrease on noise magnitude in l2 norm. Curls & Whey attack also shows promising transferability against ensemble models as well as adversarially trained models. In addition, we extend our attack to the targeted misclassification, effectively reducing the difficulty of targeted attacks under black-box condition. <br> <br>|
| ![]({{ site.url }}{{ site.baseurl }}/images/respic/adv/PDAN.png){: style="width: 350px; float: left; border: 10px"} | **2 Polishing Decision-based Adversarial Noise with a Customized Sampling** <br>Yucheng Shi, Yahong Han, Qi Tian <br>CVPR 2020 <br>In this paper, we demonstrate the advantage of using current noise and historical queries to customize the variance and mean of sampling in boundary attack to polish adversarial noise. We further reveal the relationship between the initial noise and the compressed noise in boundary attack. We propose Customized Adversarial Boundary (CAB) attack that uses the current noise to model the sensitivity of each pixel and polish adversarial noise of each image with a customized sampling setting. On the one hand, CAB uses current noise as a prior belief to customize the multivariate normal distribution. On the other hand, CAB keeps the new samplings away from historical failed queries to avoid similar mistakes. Experimental results measured on several image classification datasets emphasizes the validity of our method. <br> <br>|
| ![]({{ site.url }}{{ site.baseurl }}/images/respic/adv/aia.png){: style="width: 350px; float: left; border: 10px"} | **3 Adaptive Iterative Attack towards Explainable Adversarial Robustness** <br>Yucheng Shi, Yahong Han, Quanxin Zhang, Xiaohui Kuang <br>Pattern Recognition <br>Current iterative attacks use a fixed step size for each noise-adding step, making further investigation into the effect of variable step size on model robustness ripe for exploration. We prove that if the upper bound of noise added to the original image is fixed, the attack effect can be improved if the step size is positively correlated with the gradient obtained at each step by querying the target model. In this paper, we propose Ada-FGSM (Adaptive FGSM), a new iterative attack that adaptively allocates step size of noises according to gradient information at each step. Improvement of success rate and accuracy decrease measured on ImageNet with multiple models emphasizes the validity of our method. We analyze the process of iterative attack by visualizing their trajectory and gradient contour, and further explain the vulnerability of deep neural networks to variable step size adversarial examples. <br> <br>|





