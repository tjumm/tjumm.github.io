<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Allan Lab - Research</title>
  <meta name="description" content="Allan Lab -- Research">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="/research/">
<link rel="shortcut icon" type ="image/x-icon" href="/images/favicon.ico">



</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>
	
    <a class="navbar-brand" href="/">MM Lab @ Tianjin University</a>
	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="/">Home</a></li>
		<li><a href="/research">Research</a></li>
		<li><a href="/team">People</a></li>
		<li><a href="/publications">Publications</a></li>
		<!--<li><a href="/vacancies">Join us</a></li>-->
		<!--<li><a href="/pictures">(Pics)</a></li>-->
	  </ul>
	</div>
  </div>
</div>


    <div class="container-fluid">
      <div class="row">
        <div id="textid" class="col-sm-12">
  <h1 id="research">Research</h1>

<p>Our mainly foucus is on cross-media analysis for video such as video captioning, visual question answering and the new research topic about adversarial machine learning.</p>

<p><b> 1. Adversarial Machine Learning </b></p>

<p>Adversarial machine learning is a new research direction of our laboratory. This is a crossing research field between machine learning and computer security. The main goal is to propose more powerful and targeted attack against existing machine learning model, like image classifiers based on deep neural networks.</p>

<p><img src="/images/respic/adv.jpg" alt="" style="width: 350px; float: right; border: 10px" /></p>

<p>Adversarial attacks can be divided into white-box attack and black-box attack based on attackers’ knowledge of the target model. As for white-box attack, the attacker can gain complete knowledge of target model, including its training images, optimization algorithm, and parameters of each layer in the network. When the attacker can not access the internal structure or training images, the target model can only be regarded as a black box. In this case, the attacker can only acquire information about the target model by querying.</p>

<p>At present, our laboratory has two publications in the field of adversarial machine learning:<i>《Schmidt: Image Augmentation for Black-box Adversarial Attack》</i>that proposed an image augmentation method better probes decision boundaries of the black-box model.<i>《Universal Perturbation Generation for Black-box Attack Using Evolutionary Algorithms》</i> that achieves source/target misclassification, black-box attack and universal perturbation by employing improved evolutionary algorithms</p>

<p><b> 2. Visual Question Answering </b></p>

<p>VQA is a task that given a picture or video and a corresponding natural language question, model can generate the answer automatically. It is a multi-media task which combines the technique of Natural Language Processing(NLP) and Computer Vision(CV). It has received much attention from researchers and scholars in recent years. Solving this task is a crucial step towards Artificial Intelligence.</p>

<p><img src="/images/respic/qa.jpg" alt="" style="width: 300px; float: left; border: 10px" /></p>

<p>VQA can be divided into two categories in terms of the visual content: Image Question Answering(ImageQA) and Video Question Answering(VideoQA). As the natural extension of images, videos contain richer information with additional temporal and dynamic characteristics, which brings up more challenges and difficulties. The main aspect that our lab focuses on is how to combine the reasoning and knowledge to solve the VideoQA. When answering the complex question, human usually reason about the visual and language or combine the additional knowledge. So if we can let models to achieve such abilities, they will more intelligent.</p>

<p>The existing achievements of this topic of our lab include:
<i>《Movie Question Answering: Remembering the Textual Cues for Layered Visual Contents》(AAAI).</i>This work contributes a Layered Memory Network (LMN) to solve the MovieQA, which represents frame-level and clip-level movie content by the Static Word Memory module and the Dynamic Subtitle Memory module, respectively.
<i>《Explore Multi-step Reasoning in Video Question Answering》(ACMMM). </i>This work explores multi-step reasoning in VideoQA by formulating it as a new task, which targets to answer compositional logical structured questions bases on videos. The work contributes in developing a system to automatically generate a large-scale VideoQA dataset and proposing a novel model which combines spatial and temporal attention to address this task.</p>

<p><b> 3. Video Captioning </b></p>

</div>

      </div>
    </div>

    <div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">
		<center>
		<div class="col-sm-12">
		  Contact: Room B502, Building #55, Tianjin University (<a href="https://j.map.baidu.com/QPmWP">Maps</a>) <br />
			2018-6-1
		</center>
		</div>
	  </div>
	</div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="/js/bootstrap.min.js"></script>


  </body>

</html>
