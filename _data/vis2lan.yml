
- title: 1 Catching the Temporal Regions-of-Interest for Video Captioning
  photo: ctri.png
  author: Ziwei Yang, Yahong Han, Zheng Wang
  publish: ACM Multimedia 2017 (<b>Oral Paper and Best Paper Presentation)</b>, <a href='https://dl.acm.org/doi/abs/10.1145/3123266.3123327'>(Paper)</a> <a href='https://tend93.github.io/'>(Project Page)</a>
  abstract: Inspired by the insight that people always focus on certain interested regions of video content, we propose a novel approach which will automatically focus on regions-of-interest and catch their temporal structures. In our approach, we utilize a specific attention model to adaptively select regions-of-interest for each video frame. Then a Dual Memory Recurrent Model (DMRM) is introduced to incorporate temporal structure of global features and regions-of-interest features in parallel, which will obtain rough understanding of video content and particular information of regions-of-interest. We evaluate our method for video captioning on Microsoft Video Description Corpus (MSVD) and Montreal Video Annotation (M-VAD). The experiments demonstrate that catching temporal regions-of-interest information really enhances the representation of input videos.
  number_educ: 0

- title: 2 Video Interactive Captioning with Human Prompts
  photo: VCHP.png
  author: Aming Wu, Yahong Han, Yi Yang
  publish: IJCAI 2019, <a href='https://www.ijcai.org/proceedings/2019/135'>(Paper)</a>,  <a href='https://github.com/ViCap01/ViCap'>(Project Page)</a>
  abstract: As a video often includes rich visual content and semantic details, different people may be interested in different views. Thus the generated sentence always fails to meet the ad hoc expectations. In this paper, we make a new attempt that, we launch a round of interaction between a human and a captioning agent. After generating an initial caption, the agent asks for a short prompt from the human as a clue of his expectation. Then, based on the prompt, the agent could generate a more accurate caption. We name this process a new task of video interactive captioning (ViCap). Taking a video and an initial caption as input, we devise the ViCap agent which consists of a video encoder, an initial caption encoder, and a refined caption generator. Experimental results not only show the prompts can help generate more accurate captions, but also demonstrate the good performance of the proposed method.
  number_educ: 0

#- title: 3 Movie Question Answering: Remembering the Textual Cues for Layered Visual Contents
#  photo: mqa.png
#  author: Bo Wang, Youjiang Xu, Yahong Han, Richang Hong
#  publish: AAAI 2018, <a href='https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16359'>(Paper)</a>, <a href='https://github.com/bowong/Layered-Memory-Network'>(Project Page)</a> <br><a href='http://movieqa.cs.toronto.edu/workshops/iccv2017/'>Winner of the MovieQA and The Large Scale Movie Description Challenge (LSMDC) @ ICCV 2017</a>
#  abstract: Understanding movie stories through only visual content is still a hard problem. In this paper, for answering questions about movies, we put forward a Layered Memory Network (LMN) that represents frame-level and clip-level movie content by the Static Word Memory module and the Dynamic Subtitle Memory module, respectively. Particularly, we firstly extract words and sentences from the training movie subtitles. Then the hierarchically formed movie representations, which are learned from LMN, not only encode the correspondence between words and visual content inside frames, but also encode the temporal alignment between sentences and frames inside movie clips. We also extend our LMN model into three variant frameworks to illustrate the good extendable capabilities. The good performance successfully demonstrates that the proposed framework of LMN is effective and the hierarchically formed movie representations have good potential for the applications of movie question answering.
#  number_educ: 0

- title: 4 Connective Cognition Network for Directional Visual Commonsense Reasoning
  photo: ccn.png
  author: Aming Wu, Linchao Zhu, Yahong Han, Yi Yang
  publish: NeurIPS 2019,<a href='https://openreview.net/forum?id=rJeVcVHx8H'>(Paper)</a>, <a href='https://github.com/AmingWu/CCN'>(Project Page)</a>
  abstract: Recent studies on neuroscience have suggested that brain function or cognition can be described as a global and dynamic integration of local neuronal connectivity, which is context-sensitive to specific cognition tasks. Inspired by this idea, towards Visual commonsense reasoning (VCR), we propose a connective cognition network (CCN) to dynamically reorganize the visual neuron connectivity that is contextualized by the meaning of questions and answers. Concretely, we first develop visual neuron connectivity to fully model correlations of visual content. Then, a contextualization process is introduced to fuse the sentence representation with that of visual neurons. Finally, based on the output of contextualized connectivity, we propose directional connectivity to infer answers or rationales. Experimental results on the VCR dataset demonstrate the effectiveness of our method.
  number_educ: 0
